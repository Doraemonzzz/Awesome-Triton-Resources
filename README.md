
# Resource

1. [Official Repo](https://github.com/triton-lang/triton)
2. [Official Tutorial](https://triton-lang.org/main/index.html)
3. [Triton-Puzzles](https://github.com/srush/Triton-Puzzles)
4. [fla: Efficient implementations of state-of-the-art linear attention models in Pytorch and Triton](https://github.com/sustcsonglin/flash-linear-attention)
5. [Attorch: A subset of PyTorch's neural network modules, written in Python using OpenAI's Triton.](https://github.com/BobMcDear/attorch)
6. [Trident: A performance library for machine learning applications.](https://github.com/kakaobrain/trident)
7. [triton-transformer: Implementation of a Transformer, but completely in Triton](https://github.com/lucidrains/triton-transformer)
8. [unsloth](https://github.com/unslothai/unsloth/)
9. [Kernl](https://github.com/ELS-RD/kernl)
10. [Liger-Kernel](https://github.com/linkedin/Liger-Kernel)
11. [efficient_cross_entropy](https://github.com/mgmalek/efficient_cross_entropy)
12. [Xformers kernels](https://github.com/facebookresearch/xformers)
13. [Flash Attention kernels](https://github.com/Dao-AILab/flash-attention)
14. [FlagGems: FlagGems is an operator library for large language models implemented in Triton Language.](https://github.com/FlagOpen/FlagGems)
15. [applied-ai: Applied AI experiments and examples for PyTorch](https://github.com/pytorch-labs/applied-ai)
16. [FlagAttention: A collection of memory efficient attention operators implemented in the Triton language.](https://github.com/FlagOpen/FlagAttention)
17. [triton-activations: Collection of neural network activation function kernels for Triton Language Compiler by OpenAI](https://github.com/dtunai/triton-activations)
18. [FLASHNN](https://github.com/AlibabaPAI/FLASHNN/)
19. [GPTQ-triton](https://github.com/fpgaminer/GPTQ-triton)
20. [Accelerating Triton Dequantization Kernels for GPTQ](https://pytorch.org/blog/accelerating-triton/)
    1.  [Code](https://github.com/foundation-model-stack/foundation-model-stack/tree/triton/triton/kernels)
21. [Block-sparse matrix multiplication kernels](https://github.com/stanford-futuredata/stk)
    1.  https://openreview.net/pdf?id=doa11nN5vG
22. [bitsandbytes](https://github.com/bitsandbytes-foundation/bitsandbytes/tree/main/bitsandbytes/triton)
23. [flex-attention](https://pytorch.org/blog/flexattention/)
    1.  https://pytorch.org/blog/flexattention/
    2.  https://github.com/pytorch-labs/attention-gym
24. [conv](https://github.com/pytorch/pytorch/blob/main/torch/_inductor/kernel/conv.py)
25. [lightning_attention](https://github.com/Doraemonzzz/lightning_attention)
26. [int mm](https://github.com/pytorch/ao/blob/main/torchao/kernel/intmm_triton.py)




# Reference

This library is inspired by [Awesome-Triton-Kernels](https://github.com/zinccat/Awesome-Triton-Kernels).

